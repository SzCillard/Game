import numpy as np
import random

# ğŸŒ KÃ–RNYEZET DEFINIÃLÃSA
grid_size = 5  # 5x5-Ã¶s mÃ¡trix
start = (0, 0)  # KiindulÃ³pont
goal = (4, 4)   # CÃ©l
obstacles = [(1, 1), (2, 2), (4, 1)]  # AkadÃ¡lyok

# ğŸ“Œ Ãllapotok Ã©s akciÃ³k
actions = ["up", "down", "left", "right"]
q_table = np.zeros((grid_size, grid_size, len(actions)))  # Q-tÃ¡bla inicializÃ¡lÃ¡sa

# Map to print
def print_map():
    for i in range(grid_size):
        for j in range(grid_size):
            if (i, j) == start:
                print("ğŸŸ¢", end=" ")
            elif (i, j) == goal:
                print("ğŸ”µ", end=" ")
            elif (i, j) in obstacles:
                print("âš«ï¸", end=" ")
            else:
                print("âšªï¸", end=" ")
        print()

# ğŸ¯ JutalmazÃ¡si rendszer
def get_reward(state):
    if state == goal:
        return 10  # Nagy jutalom, ha elÃ©rte a cÃ©lt
    elif state in obstacles:
        return -10  # BÃ¼ntetÃ©s, ha akadÃ¡lyba Ã¼tkÃ¶zik
    else:
        return -1  # Kis negatÃ­v jutalom (hogy Ã¶sztÃ¶nÃ¶zze a gyors mozgÃ¡st)

# ğŸ”„ AkciÃ³k vÃ©grehajtÃ¡sa
def take_action(state, action):
    x, y = state
    if action == "up":
        x = max(0, x - 1)
    elif action == "down":
        x = min(grid_size - 1, x + 1)
    elif action == "left":
        y = max(0, y - 1)
    elif action == "right":
        y = min(grid_size - 1, y + 1)
    return (x, y)

# ğŸ† Q-learning algoritmus
alpha = 0.1  # TanulÃ¡si rÃ¡ta
gamma = 0.9  # JÃ¶vÅ‘beli jutalom fontossÃ¡ga
epsilon = 0.1  # VÃ©letlenszerÅ± felfedezÃ©s arÃ¡nya

for episode in range(1000):  # 1000 tanÃ­tÃ¡si epizÃ³d
    state = start
    while state != goal:
        if random.uniform(0, 1) < epsilon:
            action = random.choice(actions)  # VÃ©letlenszerÅ± akciÃ³ (felfedezÃ©s)
        else:
            action = actions[np.argmax(q_table[state[0], state[1]])]  # Legjobb ismert akciÃ³

        new_state = take_action(state, action)
        reward = get_reward(new_state)

        # ğŸ“Œ Q-tÃ¡bla frissÃ­tÃ©se (Q-learning szabÃ¡ly)
        q_table[state[0], state[1], actions.index(action)] = (
            (1 - alpha) * q_table[state[0], state[1], actions.index(action)]
            + alpha * (reward + gamma * np.max(q_table[new_state[0], new_state[1]]))
        )

        state = new_state  # LÃ©pjÃ¼nk az Ãºj Ã¡llapotba

# ğŸ“Œ KiÃ­ratjuk a tanult Q-tÃ¡blÃ¡t
print("Tanult Q-tÃ¡bla:")
print_map()
print(q_table)
